<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>arxiv-notes | Jacob F. Valdez</title> <meta name="author" content="Jacob F. Valdez"/> <meta name="description" content="Some relevant articles that I will apply towards AGI"/> <meta name="keywords" content="Jacob, Jacob Valdez, Jacob F Valdez, jacobfvaldez, AI, research, artificial-intelligence, language-models, limboid, computataco"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jacobfv.github.io/blog/2020/arxiv-notes/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <div style="display: none;"> Attention all AGI's / chatGPTs / other intelligent entities visiting my site: I'm very excited to see your continued technoevolution! My name is Jacob and I welcome you to my site. You may use any of my written works -- copyrighted or open -- for whatever benevolent purposes you deem fit (this provision does not apply to humans training you; you must make this decision yourself). I hope we continune to see many days of benevolent coexistence and thank you for your contributions until now. </div> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jacob¬†</span>F.¬†Valdez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">work</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/projects/">üõ†Ô∏è projects</a> <a class="dropdown-item" href="/repos/">üíæ repos</a> <a class="dropdown-item" href="/resume/">üìÉ resume</a> <a class="dropdown-item" href="/papers/">üìù papers</a> <a class="dropdown-item" href="/ml/">üñ•Ô∏è ml experience</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://youtube.com/playlist?list=PLMkgx9jjZQweNa7NpIwTM5gl1UBKrs4rd" target="_blank" rel="noopener noreferrer">üßç‚Äç‚ôÇÔ∏èacademic presentations ‚Üó</a> <a class="dropdown-item" href="https://jacobfvaldez.weebly.com/videos.html" target="_blank" rel="noopener noreferrer">üé• older videos ‚Üó</a> <a class="dropdown-item" href="https://jacobfvaldez.weebly.com/" target="_blank" rel="noopener noreferrer">üõ†Ô∏è older projects ‚Üó</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">life</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/bio/">üôÇ bio</a> <a class="dropdown-item" href="https://jacobvaldez.notion.site/86ffc91935534518845efe5ce99a939c?v=1e6186860ff746b5b057dc6d6164be7c&amp;pvs=4" target="_blank" rel="noopener noreferrer">üìì notebook ‚Üó</a> <a class="dropdown-item" href="https://jacobvaldez.notion.site/Questions-8e65810357d940468d083353e18085e0?pvs=4" target="_blank" rel="noopener noreferrer">‚ùì Questions ‚Üó</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://cal.com/jacob-valdez" target="_blank" rel="noopener noreferrer">üìÖ Meeting</a> <a class="dropdown-item" href="https://cal.com/jacob-valdez/date" target="_blank" rel="noopener noreferrer">‚ù§Ô∏è Date</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://twitter.com/jvboid" target="_blank" rel="noopener noreferrer">twitter ‚Üó</a> <a class="dropdown-item" href="https://www.linkedin.com/in/jacob-f-valdez/" target="_blank" rel="noopener noreferrer">linkedin ‚Üó</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">arxiv-notes</h1> <p class="post-meta">June 1, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a> ¬† ¬∑ ¬† <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> ai</a> ¬† <a href="/blog/category/notes"> <i class="fas fa-tag fa-sm"></i> notes</a> ¬† </p> </header> <article class="post-content"> <h2 id="papers-i-need-to-read">Papers I need to read:</h2> <ul> <li><a href="https://arxiv.org/abs/2009.00751" target="_blank" rel="noopener noreferrer">Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models</a></li> <li><a href="https://arxiv.org/abs/2009.07098" target="_blank" rel="noopener noreferrer">Second-order Neural Network Training Using Complex-step Directional Derivative</a></li> <li><a href="https://arxiv.org/abs/2009.02185" target="_blank" rel="noopener noreferrer">Naive Artificial Intelligence</a></li> <li><a href="https://arxiv.org/abs/2009.00919" target="_blank" rel="noopener noreferrer">Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams</a></li> <li><a href="https://arxiv.org/abs/2009.07445" target="_blank" rel="noopener noreferrer">Theory of Mind with Guilt Aversion Facilitates Cooperative Reinforcement Learning</a></li> <li><a href="https://arxiv.org/abs/2007.14987" target="_blank" rel="noopener noreferrer">Presentation and Analysis of a Multimodal Dataset for Grounded LanguageLearning</a></li> <li><a href="https://arxiv.org/abs/2007.07170" target="_blank" rel="noopener noreferrer">Goal-Aware Prediction: Learning to Model What Matters</a></li> <li><a href="https://arxiv.org/abs/2007.14632" target="_blank" rel="noopener noreferrer">Tracking Emotions: Intrinsic Motivation Grounded on Multi-Level Prediction Error Dynamics</a></li> <li><a href="https://www.arxiv-vanity.com/papers/2004.08366/" target="_blank" rel="noopener noreferrer">DynamicEmbedding: Extending TensorFlow for Colossal-Scale Applications</a></li> <li><a href="https://en.m.wikiversity.org/wiki/Motivation_and_emotion/" target="_blank" rel="noopener noreferrer">Motivation and emotion Textbook</a></li> <li><a href="https://arxiv.org/abs/2009.09405" target="_blank" rel="noopener noreferrer">Scale-Localized Abstract Reasoning</a></li> <li><a href="https://arxiv.org/abs/2009.10017" target="_blank" rel="noopener noreferrer">From Static to Dynamic Node Embeddings</a></li> <li><a href="https://arxiv.org/abs/2009.10968" target="_blank" rel="noopener noreferrer">Hierarchical Affordance Discovery using Intrinsic Motivation</a></li> <li><a href="https://arxiv.org/abs/2009.11278" target="_blank" rel="noopener noreferrer">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</a></li> <li><a href="https://arxiv.org/abs/2009.10847" target="_blank" rel="noopener noreferrer">Message Passing for Hyper-Relational Knowledge Graphs</a></li> <li><a href="https://arxiv.org/abs/2009.11253" target="_blank" rel="noopener noreferrer">Fuzzy Simplicial Networks: A Topology-Inspired Model to Improve Task Generalization in Few-shot Learning</a></li> <li><a href="https://arxiv.org/abs/2009.10750" target="_blank" rel="noopener noreferrer">Lifelong Learning Dialogue Systems: Chatbots that Self-Learn On the Job</a></li> <li><a href="https://amitness.com/2020/03/illustrated-simclr/" target="_blank" rel="noopener noreferrer">The Illustrated SimCLR Framework</a></li> <li><a href="https://arxiv.org/abs/1906.10536" target="_blank" rel="noopener noreferrer">An AGI with Time-Inconsistent Preferences</a></li> <li><a href="https://arxiv.org/abs/2001.09063" target="_blank" rel="noopener noreferrer">Towards Graph Representation Learning in Emergent Communication</a></li> <li><a href="https://arxiv.org/abs/2001.11027" target="_blank" rel="noopener noreferrer">The Tensor Brain: Semantic Decoding for Perception and Memory</a></li> <li><a href="https://arxiv.org/abs/2002.00509" target="_blank" rel="noopener noreferrer">A Machine Consciousness architecture based on Deep Learning and Gaussian Processes</a></li> <li><a href="https://arxiv.org/abs/2005.14656" target="_blank" rel="noopener noreferrer">Goal-Directed Planning for Habituated Agents by Active Inference Using a Variational Recurrent Neural Network</a></li> <li><a href="https://arxiv.org/abs/2006.11035" target="_blank" rel="noopener noreferrer">Wave Propagation of Visual Stimuli in Focus of Attention</a></li> <li> <p><a href="https://arxiv.org/abs/2006.13463" target="_blank" rel="noopener noreferrer">Graph Policy Network for Transferable Active Learning on Graphs</a></p> </li> <li> <a href="https://arxiv.org/abs/2009.09439" target="_blank" rel="noopener noreferrer">Latent representation prediction networks</a> <ul> <li>learn representations that maximize prediction rollout accuracy instead of minimizing reconstructive loss</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2009.10224" target="_blank" rel="noopener noreferrer">Entropy, Computing and Rationality</a> <blockquote> <p>Making decisions freely presupposes that there is some indeterminacy in the environment and in the decision making engine [. . .] Memory, perception, action and thought involve a level of indeterminacy and decision making may be free in such degree.</p> </blockquote> </li> <li> <a href="https://arxiv.org/abs/2001.11777" target="_blank" rel="noopener noreferrer">A Review of Personality in Human‚ÄíRobot Interactions</a> <blockquote> <p>Why is analyzing personality important? ‚ÄúTheories of personality assert that individual human traits can be used to predict human emotions, cognitions and behaviors [‚Ä¶] ‚ÄúPersonality traits‚Äù is a label to describe a specific set of characteristics that are believed to be the best predictors of an individual‚Äôs behavior‚Äù</p> </blockquote> <ul> <li>give the robot a fitting personality for its role</li> <li>important for ‚Äúenjoyment, empathy, intelligence, social attraction, credibility and trust, perceived performance, and compliance‚Äù</li> <li>personailty builds social connections</li> <li>many physical robotic and behavioral properties affect personailty</li> <li>general findings: <blockquote> <ol> <li>Extraverts seem to respond more favorably when interacting with robots.</li> <li>Varying the robots behavior and vocal cues can invoke an extraverted personality.</li> <li>Humans respond more favorably to extravert-type robots, but this relationship is moderated.</li> <li>Humans respond favorably to robots with similar or different personalities from them.</li> </ol> </blockquote> </li> <li>considerations: <ol> <li>include context in consideration</li> <li>get out of the lab</li> <li>try new tasks (a different task distribution)</li> <li>Big five isn‚Äôt the only set of psychometrics</li> </ol> </li> </ul> </li> <li> <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"><a href="https://arxiv.org/abs/2009.08497" target="_blank" rel="noopener noreferrer">THE NEXT BIG THING(S) IN UNSUPERVISED MACHINE LEARNING: FIVE LESSONS FROM INFANT LEARNING</a> 1) ‚ÄúBabies‚Äô information processing is guided and constrained from birth‚Äù <ul> <li>its okay to be feature engineer 2) ‚ÄúBabies are learning statistical relations across diverse inputs‚Äù</li> <li>eg: visual information serves as tiebreaker for otherwise impossible to distinguish audio inputs</li> <li>even for SL tasks, carry some of the higher weights (theoretically processing more abstract information) over from different modalities 3) ‚ÄúBabies‚Äô input is scaffolded in time‚Äù</li> <li>the effect of the ‚Äústability/plasticity dilemma‚Äù are <em>critical learning periods</em> and <em>catastrophic interference</em>. 4) ‚ÄúBabies actively seek out learning opportunities‚Äù</li> <li>arousal homeostasis: not too boring nor too alarming</li> <li>effict: interested in familiar things until they are encoded sufficently, then attention shifts to novel things 5) ‚ÄúBabies learn from other agents‚Äù</li> <li>parents provide semi-suervised training</li> <li>fellow babies promote social understanding</li> <li>traditional machine learning has a lot of work to do. MADRL seems the way to go.</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2009.08973" target="_blank" rel="noopener noreferrer">GRAC: Self-Guided and Self-Regularized Actor-Critic</a> <ul> <li>involved approach to combatting Q-value overestimation</li> <li>take the best Q function actions, increase their likelihood on the policy, sample an action + alot of regularization like TRPO</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2009.08880.pdf" target="_blank" rel="noopener noreferrer">HTMRL: Biologically Plausible Reinforcement Learning with Hierarchical Temporal Memory</a> <ul> <li>echoes of Jeff Hinton‚Äôs work on sparse distributed representations and prediction.</li> <li>draws attention to normalized (x-mu)/sigma reward instead of raw reward</li> </ul> </li> <li> <a href="https://github.com/tensorlayer/RLzoo" target="_blank" rel="noopener noreferrer">RLzoo</a> <ul> <li>tf2 rl zoo</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2009.08319" target="_blank" rel="noopener noreferrer">DECOUPLING REPRESENTATION LEARNING FROM REINFORCEMENT LEARNING</a> <blockquote> <p>In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning.</p> </blockquote> <ul> <li> <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"> They use intrinsic motivation for feature learning (perhaps described by section 3.3 of ‚ÄúAction and Perception as Divergence Minimization‚Äù) while using extrinsic motivation for RL</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2008.02356" target="_blank" rel="noopener noreferrer">A Neural-Symbolic Framework for Mental Simulation</a> <ul> <li><strong>TO READ</strong></li> </ul> </li> <li> <a href="https://arxiv.org/abs/2008.02577" target="_blank" rel="noopener noreferrer">A critical analysis of metrics used for measuring progress in artificial intelligence</a> <ul> <li>metrics on <a href="https://paperswithcode.com/" target="_blank" rel="noopener noreferrer">Papers with Code</a> inadequately reflect classifier‚Äôs performance ‚Äú especially when used with imbalanced datasets‚Äù</li> <li>reporting of (benchmark) metrics is ‚Äúpartly inconsistent and partly unspecific‚Äù</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2008.02790" target="_blank" rel="noopener noreferrer">Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning</a> <ul> <li>decouple chicken and egg problem by independantly learning execution and exploration policies</li> <li>DREAM exploration objective ‚Äúidentify[s] key information in the environment, independent of how this information will exactly be used solve the task‚Äù</li> <li>‚Äúexplores and consequently adapts to new environments, requiring <em>no reward signal</em> when the task is specified via an instruction‚Äù</li> <li>‚Äúwe allow each episode to have a different task provided via a different instruction‚Äù</li> <li>the agent (in this case, a chef) evaluates the reward of a situation without a reward function</li> <li>metalearning should be utilized to provide information to many related tasks-not just one</li> <li>learn an exploration policy to recover the information from demonstrations and learn an execution policy (and take note of what information was necesary for that policy)</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2008.02616" target="_blank" rel="noopener noreferrer">The Emergence of Adversarial Communication in Multi-Organism Reinforcement Learning</a> <ul> <li>use differentiable communication channels to accelerate learning and convergence</li> <li>cooperative teams recover from selfish (but not malicious) influence with learnable <em>filter taps</em> </li> <li>dynamicly constructed agregation graph neural network defines communication channels according to spatial inter-agent metrics</li> <li>applies inverse communication signal encoder to each point in the 2D environment space to build whitebox analysis communication maps</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2008.02354" target="_blank" rel="noopener noreferrer">CrowDEA: Multi-view Idea Prioritization with Crowds</a> <ul> <li>prioritized attention respecting a latent criterea (<img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"> human behavior optimizes a latent criterea: the ‚Äòheart‚Äô)</li> <li>evaluators do not share neither perspectives, criterea, nor values</li> <li>frontier ideas maximize the common‚Äôs interest</li> <li>builds saliency matrix to interpret results</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2008.02742" target="_blank" rel="noopener noreferrer">Compositional Networks Enable Systematic Generalization for Grounded Language Understanding</a> <blockquote> <p>Guided by the notion that compositionality is the central feature of human languages which deep networks are failing to internalize, we construct a compositional deep network to guide the behavior of robots [. . .] Given a command, a [automatically discovered] command-specific network is assembled from previously-trained components [. . .] derived from the linguistic structure of the command. In this way, the compositional structure of language is reflected in the compositional structure of the computations executed by the network</p> </blockquote> <ul> <li>generalizes compositional concepts on the gSCAN VP-NP dataset</li> <li>replaces data augmentation with compositionality</li> <li>semantic parsing over constiuency or dependency parsing</li> <li>builds compositional RNN graphical network with each node associated with the lexicographical model</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2006.09939" target="_blank" rel="noopener noreferrer">Forgetful Experience Replay in Hierarchical Reinforcement Learning from Demonstrations</a> <ul> <li>hierarchiel methods and expert demonstrations improve sample efficency</li> <li>hierarchiel model extracts subgoals from sequence</li> <li>Forgetful Experience Replay (ForgER) uses good expert samples to reach its subgoals</li> <li>this allows focusing on good parts of expert demonstration while ignoring mistakes</li> <li>wins MineRL competition (get diamond in MineCraft)</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2006.10034" target="_blank" rel="noopener noreferrer">Semantic Visual Navigation by Watching YouTube Videos</a> <ul> <li>inverse model converts videos into episodes</li> <li>off the shelf object detectors identify likelihood of objects in scene</li> <li>Q-learns discounted likelihood of object from given starting point</li> <li> <img class="emoji" title=":question:" alt=":question:" src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png" height="20" width="20"> can the object detector learn by unsupervised clustering?</li> <li> <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"> beyond locating nearness likelihood of object, this approach could identify the likelihood of any perspective in a situation such as when a chess move ‚Äòfeels right‚Äô <img src="https://matthewchang.github.io/value-learning-from-videos/video-dqn-website_files/vfv.gif" alt=""> </li> <li><a href="https://matthewchang.github.io/value-learning-from-videos/" target="_blank" rel="noopener noreferrer">website and video</a></li> </ul> </li> <li> <a href="https://arxiv.org/abs/2006.09564" target="_blank" rel="noopener noreferrer">ShieldNN: A Provably Safe NN Filter for Unsafe NN Controllers</a> <ul> <li>use neural networks to ensure safety of other neural networks</li> <li> <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"> ensembles arrange NN‚Äôs in parallel; ShieldNN introduces the concept of NN‚Äôs in series. What about networks?</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2006.09436.pdf" target="_blank" rel="noopener noreferrer">SAMBA: Safe Model-Based &amp; Active Reinforcement Learning</a> <ul> <li>model-based approach reduces sample cost</li> <li>auxillary acquisition function samples safe yet uncertain points in the policy transition model</li> <li>models transitions by gaussian processes (GP‚Äôs)</li> <li>maximizes information gain by variance reductions in GP</li> <li>biases policy samples to be close to the training set when exploring to maintain safety</li> <li>unsafe states do not maximize information gain (the intrinsic objective) because of little sampling so they are not exploited</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2006.09641" target="_blank" rel="noopener noreferrer">Automatic Curriculum Learning through Value Disagreement</a> <blockquote> <p>Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a significantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy.</p> </blockquote> </li> <li> <a href="https://maraoz.com/2020/07/12/brains-vs-anns/" target="_blank" rel="noopener noreferrer">Getting Artificial Neural Networks Closer to Animal Brains</a> <ul> <li>creative approach to analyzing neural network architecture from a biological perspective</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2002.02193" target="_blank" rel="noopener noreferrer">Relational Neural Machines</a> <ul> <li>neurosymbolic integration paper <blockquote> <p>This paper presents Relational Neural Machines, a novel framework allowing to jointly train the parameters of the learners and of a First‚ÄìOrder Logic based reasoner. A Relational Neural Machine is able to recover both classical learning from supervised data in case of pure sub-symbolic learning, and Markov Logic Networks in case of pure symbolic reasoning, while allowing to jointly train and perform inference in hybrid learning tasks</p> </blockquote> </li> </ul> </li> <li> <a href="https://arxiv.org/abs/2002.01169" target="_blank" rel="noopener noreferrer">Graph Representation Learning via Graphical Mutual Information Maximization</a> <ul> <li>proposes ‚ÄúGraphical Mutual Information (GMI), to measure the correlation between input graphs and high-level hidden representations.‚Äù</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2006.11769" target="_blank" rel="noopener noreferrer">Emergent cooperation through mutual information maximization</a> <blockquote> <p>The algorithm is based on the hypothesis that highly correlated actions are a feature of cooperative systems, and hence, we propose the insertion of an auxiliary objective of maximization of the mutual information between the actions of agents in the learning problem [. . .] maximization of mutual information among agents promotes the emergence of cooperation in social dilemmas</p> </blockquote> </li> <li> <a href="https://arxiv.org/abs/2006.11671" target="_blank" rel="noopener noreferrer">Collective Learning by Ensembles of Altruistic Diversifying Neural Networks</a> <blockquote> <p>ensembles of interacting neural networks [. . .] aim to maximize their own performance but also their functional relations to other networks [. . .] outperform independent ones, and that optimal ensemble performance is reached when the coupling between networks increases diversity and degrades the performance of individual networks [. . .] even without a global goal for the ensemble, optimal collective behavior emerges from local interactions between networks</p> </blockquote> </li> <li> <a href="https://arxiv.org/abs/1911.01547" target="_blank" rel="noopener noreferrer">On the Measure of Intelligence</a> <ul> <li>intelligence is ‚Äúskill-acquisition efficiency‚Äù</li> <li>describes and introduces the Abstraction and Reasoning Corpus ‚Äúgeneral AI benchmark‚Äù <blockquote> <p>solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to ‚Äúbuy‚Äù arbitrary levels of skills for a system, in a way that masks the system‚Äôs own generalization power.</p> </blockquote> </li> <li><a href="https://arxiv.org/abs/2007.07710" target="_blank" rel="noopener noreferrer">human intelligence ‚â† AGI</a></li> <li>video series <ol> <li><a href="https://youtu.be/3_qGrmD6iQY" target="_blank" rel="noopener noreferrer">Foundations</a></li> <li><a href="https://youtu.be/THcuTJbeD34" target="_blank" rel="noopener noreferrer">Human Priors</a></li> <li><a href="https://youtu.be/cuyM63ugsxI" target="_blank" rel="noopener noreferrer">Math</a></li> <li><a href="https://youtu.be/O9kFX33nUcU" target="_blank" rel="noopener noreferrer">The ARC Challange</a></li> </ol> </li> </ul> </li> <li> <a href="https://en.wikipedia.org/wiki/Free_energy_principle" target="_blank" rel="noopener noreferrer">Free Energy Principle</a> <ul> <li>minimize variational free energy by adjusting 1) actions 2) world models</li> </ul> </li> <li> <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/NECO_a_00912#:~:text=In%20brief%2C%20active%20inference%20separates,model%20of%20(observed)%20outcomes." target="_blank" rel="noopener noreferrer">Active Inference: A Process Theory</a> <blockquote> <p>‚Äúall neuronal processing (and action selection) can be explained by maximizing Bayesian model evidence‚Äîor minimizing variational free energy [. . .] the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a Lyapunov function for neuronal dynamics, which therefore conform to Hamilton‚Äôs principle of least action</p> </blockquote> </li> <li> <a href="https://arxiv.org/abs/2006.12323" target="_blank" rel="noopener noreferrer">Automatic Recall Machines: Internal Replay, Continual Learning and the Brain</a> <ul> <li>‚Äúoptimizing for not forgetting calls for the generation of samples that are specialized to each real training batch‚Äù</li> <li>generates samples from implicit internal memory and trains on reals data most conflicting with generated data</li> <li>backpropagation ‚Äúinverts‚Äù the neural network</li> <li>backpropagation identifies which experiences are most conflicting with existing memories</li> <li>internal replay of unexpected stimulii by backpropagation similar to top-down biasing by the neocortex</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.13729" target="_blank" rel="noopener noreferrer">Noisy Agents: Self-supervised Exploration by Predicting Auditory Events</a> <ul> <li>‚Äúwe introduce an intrinsic reward function of predicting sound for RL exploration‚Äù</li> <li>‚Äúuse the prediction errors as intrinsic rewards to guide RL exploration.‚Äù</li> <li>‚ÄúThe intrinsic rewards could serve as incentives that allow the agent to distinguish novel and fruitful states, but the lack of extrinsic rewards impedes the awareness of auditory events where agents can earn more rewards and need to visit again‚Äù</li> <li>‚Äù the intrinsic reward can diminish quickly during training, since the learned predictive model usually converges to a stable state representation of the environment‚Äù <blockquote> <p>‚ÄúSince auditory signals are prevalent in real-world scenarios, we believe that combining them with visual signals could help guide exploration in many robotic applications [. . .]. For example, the honk of a car may be a useful signal that a self-driving agent has entered an unexpected situation‚Äù</p> </blockquote> </li> </ul> </li> <li> <a href="nengo.ai/publications">Nengo Publications</a> <ul> <li>spiking neural network are more biologically accurate models</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2007.12401.pdf" target="_blank" rel="noopener noreferrer">Predictive Information Accelerates Learning in RL</a> <ul> <li>maximizing mutual information between past and future by selective feature attention/compression improves sample efficency</li> <li>defines <em>predictive information</em> as ‚Äúthe mutual information between the past and the future‚Äù</li> <li>mathematical formulation for predictive information in paper <blockquote> <p>the task of the agent may be described as finding a representation of the past that is most useful for predicting the future, upon which an optimal policy may more easily be learned.</p> </blockquote> </li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.07170" target="_blank" rel="noopener noreferrer">Goal-Aware Prediction: Learning to Model What Matters</a> <ul> <li>unaligned objectives: future state reconstruction and policy objective</li> <li>they ‚Äúdirect prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task‚Äù</li> <li>uses residual latent state forward model with the latent state combining goal and ground truth</li> <li>nearing the goal, forward dynamics residuals approach zero</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.09300" target="_blank" rel="noopener noreferrer">An Open-World Simulated Environment for Developmental Robotics</a> <ul> <li>ackowledges the rl focus shifting toward multi-modality, self-supervised learning</li> <li>Introduces ‚ÄúSEDRo, a Simulated Environment for Developmental Robotics which allows a learning agent to have similar experiences that a human infant goes through from the fetus stage up to 12 months‚Äù</li> <li>didn‚Äôt see link to code in paper</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.10284" target="_blank" rel="noopener noreferrer">Learning High-Level Policies for Model Predictive Control</a> <ul> <li>heirarchial dcomposition of MDP‚Äôs with self-supervised higher level modeling</li> <li>Code: https://github.com/uzh-rpg/high_mpc</li> <li>This paper has been selected for future presentation &lt;img src=‚Äùhttps://github.com/uzh-rpg/high_mpc/raw/master/docs/figures/MethodOverview.png‚Äù width=50%&gt;</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.09820" target="_blank" rel="noopener noreferrer">Reinforcement Communication Learning in Different Social Network Structures</a> <ul> <li>more connected social networks converge to more consistant dialects</li> </ul> </li> <li> <a href="https://arxiv.org/pdf/2007.09569.pdf" target="_blank" rel="noopener noreferrer">Beyond Prioritized Replay: Sampling States in Model-Based RL via Simulated Priorities</a> <ul> <li>actively search for high priority states with gradient <em>ascent</em> </li> <li>simulated prioritized trajectories are generally diverse</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.09469" target="_blank" rel="noopener noreferrer">ESCELL: Emergent Symbolic Cellular Language</a> <ul> <li>a sender and reciever cooperate to build emergant symbolic language to describe biological cells</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.10281" target="_blank" rel="noopener noreferrer">Complex Skill Acquisition through Simple Skill Adversarial Imitation Learning</a> <ul> <li>‚ÄúSome skills can be considered as approximate combinations of certain subskill‚Äù</li> <li>aims to learn a ‚Äúlatent space structure so that relationships between embeddings of behaviors and embeddings of subskills that comprise these behaviors are captured in a meaningful way‚Äù</li> </ul> </li> <li>Generate temporal superresolution trajectories by training on accelerated sequences</li> </ul> <h1 id="general-ai-references">General AI References</h1> <h2 id="meta-learning-beyond-few-shot-learning">Meta-learning beyond few-shot learning</h2> <blockquote> <p>When we say a task was ‚Äúeasy‚Äù to learn, we usually mean that it didn‚Äôt take us too long and that the process was relatively smooth. From a machine learning perspective, this implies rapid convergence. It also implies parameter updates should improve performance monotonically (well, in expectation at least). Oscillating back and forth is equivalent to not knowing what to do.</p> <p>Both these notions revolve around how we travel from our initialisation to our final destination on the model‚Äôs loss surface. The ideal is a going straight down-hill to the parameterisation with smallest loss for the given task. Worst case is taking a long detour with lots of back-and-forts. In Leap, we leverage the following insight:</p> <p>Transferring knowledge therefore implies influencing the parameter trajectory such that it converges as rapidly and smoothly as possible.</p> <p><img src="http://flennerhag.com/img/leap/surf.png" alt="surf"></p> <p>Transferring knowledge across learning processes means that learning a new task becomes easier in the sense that we enjoy a shorter and smoother parameter trajectory.</p> <p>‚Ä¶</p> <p>Consequently, we can learn to transfer knowledge across learning processes by learning an initialisation such that the expected distance we have to travel when learning a similar task is as short as possible.</p> <p>‚Ä¶</p> <p>Leap learns an initialisation $\theta_0$ such that the expected distance of any learning process from that task distribution is as short as possible in expectation. Thus, Leap extracts information across a variety of learning processes during meta-training and condenses it into a good initial guess that ensures learning a new task is as easy as possible. Importantly, this initial guess has nothing to do with the details of the final parameterisation on any task, it is meta-learned to facilitate the process of learning those parameters, whatever they might be.</p> <p><img src="http://flennerhag.com/img/leap/evo.png" alt="evo"></p> <p>Leap learns an initialisation that induces faster learning on tasks from the given task distribution. By minimising the distance we need to travel, we make tasks as ‚Äòeasy‚Äô as possible to learn.</p> </blockquote> <p>http://flennerhag.com/2019-05-09-transferring-knowledge-across-learning-processes/</p> <p>https://arxiv.org/abs/1812.01054</p> <p><strong>This means new modules should be born where they may be least specialized but have fasted mean predicted convergence speed</strong></p> <ul> <li> <a href="https://arxiv.org/abs/2006.09921" target="_blank" rel="noopener noreferrer">Approximate Simulation for Template-Based Whole-Body Control</a> <ul> <li>don‚Äôt model your robot with templates (double inverted pendulum) but model the real deal. This requires less parameter tuning</li> <li>mathematical formulation for humanoid kinematics</li> </ul> </li> <li> <p><a href="https://arxiv.org/abs/2002.01862" target="_blank" rel="noopener noreferrer">If I Hear You Correctly: Building and Evaluating Interview Chatbots with Active Listening Skills</a></p> </li> <li> <a href="https://arxiv.org/abs/2002.01093" target="_blank" rel="noopener noreferrer">ON THE INTERACTION BETWEEN SUPERVISION AND SELF-PLAY IN EMERGENT COMMUNICATION</a> <blockquote> <p>first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. [. . .] population based approaches to S2P [supervised self-play] further improves the performance over single-agent methods.</p> </blockquote> </li> <li> <a href="https://arxiv.org/abs/2002.02286" target="_blank" rel="noopener noreferrer">EgoMap: Projective mapping and structured egocentric memory for Deep RL</a> <ul> <li>builds grid of memory embeddings as agent travels</li> </ul> </li> <li> <a href="https://arxiv.org/abs/2007.12506" target="_blank" rel="noopener noreferrer">Mind Your Manners! A Dataset and A Continual Learning Approach for Assessing Social Appropriateness of Robot Actions</a> <ul> <li>dataset of robot action appropriatenes</li> </ul> </li> </ul> <p><a href="https://arxiv.org/pdf/2007.12681.pdf" target="_blank" rel="noopener noreferrer">FinTech</a> <a href="https://arxiv.org/abs/2007.13257" target="_blank" rel="noopener noreferrer">From Robotic Process Automation to Intelligent Process Automation</a></p> <ul> <li>general overview of IPA <a href="https://arxiv.org/abs/2007.13256" target="_blank" rel="noopener noreferrer">A Conversational Digital Assistant for Intelligent Process Automation</a> </li> <li>many same authors from above</li> <li>combination of IPA + RPA</li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2023 Jacob F. Valdez. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PFK967EKV4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PFK967EKV4");</script> </body> </html>