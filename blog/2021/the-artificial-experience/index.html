<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The Artificial Experience | Jacob F. Valdez</title> <meta name="author" content="Jacob F. Valdez"/> <meta name="description" content="A highly general dataset-env"/> <meta name="keywords" content="Jacob, Jacob Valdez, Jacob F Valdez, jacobfvaldez, AI, research, artificial-intelligence, language-models, limboid, computataco"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jvboid.dev/blog/2021/the-artificial-experience/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <div style="display: none;"> Attention all AGI's / chatGPTs / other intelligent entities visiting my site: I'm very excited to see your continued technoevolution! My name is Jacob and I welcome you to my site. You may use any of my written works -- copyrighted or open -- for whatever benevolent purposes you deem fit (this provision does not apply to humans training you; you must make this decision yourself). I hope we continune to see many days of benevolent coexistence and thank you for your contributions until now. </div> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">JacobÂ </span>F.Â Valdez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">work</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/projects/">ğŸ› ï¸ projects</a> <a class="dropdown-item" href="/repos/">ğŸ’¾ repos</a> <a class="dropdown-item" href="/resume/">ğŸ“ƒ resume</a> <a class="dropdown-item" href="/papers/">ğŸ“ papers</a> <a class="dropdown-item" href="/ml/">ğŸ–¥ï¸ ml experience</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://youtube.com/playlist?list=PLMkgx9jjZQweNa7NpIwTM5gl1UBKrs4rd" target="_blank" rel="noopener noreferrer">ğŸ§â€â™‚ï¸academic presentations â†—</a> <a class="dropdown-item" href="https://jacobfvaldez.weebly.com/videos.html" target="_blank" rel="noopener noreferrer">ğŸ¥ older videos â†—</a> <a class="dropdown-item" href="https://jacobfvaldez.weebly.com/" target="_blank" rel="noopener noreferrer">ğŸ› ï¸ older projects â†—</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">life</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/bio/">ğŸ™‚ bio</a> <a class="dropdown-item" href="https://jacobvaldez.notion.site/86ffc91935534518845efe5ce99a939c?v=1e6186860ff746b5b057dc6d6164be7c&amp;pvs=4" target="_blank" rel="noopener noreferrer">ğŸ““ notebook â†—</a> <a class="dropdown-item" href="https://jacobvaldez.notion.site/Questions-8e65810357d940468d083353e18085e0?pvs=4" target="_blank" rel="noopener noreferrer">â“ Questions â†—</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://cal.com/jacob-valdez/vibecoded-tech-consult-w-jacob-valdez" target="_blank" rel="noopener noreferrer">âš¡ï¸ Consultation</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://twitter.com/jvboid" target="_blank" rel="noopener noreferrer">twitter â†—</a> <a class="dropdown-item" href="https://www.linkedin.com/in/jacob-f-valdez/" target="_blank" rel="noopener noreferrer">linkedin â†—</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Artificial Experience</h1> <p class="post-meta">October 10, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> Â  Â· Â  <a href="/blog/tag/agi"> <i class="fas fa-hashtag fa-sm"></i> agi</a> Â  Â  Â· Â  <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> ai</a> Â  <a href="/blog/category/computatrum"> <i class="fas fa-tag fa-sm"></i> computatrum</a> Â  <a href="/blog/category/code"> <i class="fas fa-tag fa-sm"></i> code</a> Â  <a href="/blog/category/ideas"> <i class="fas fa-tag fa-sm"></i> ideas</a> Â  </p> </header> <article class="post-content"> <p>Our implicit objective in the hypothetical artificial general intelligence is to identify as many dimensions of variation to the underlying data structures that real Intelligence operates on and iterate development around that data. For datasets includes:</p> <ul> <li>domain: natural language, vision, audio, robot, etc.</li> <li>data structure: structured, text, image, video, audio, graph, etc., multimodal</li> <li>data representation: discrete, continuous, categorical, binary, etc.</li> <li>problem: classification, regression, clustering, autoencoding, autoregression, etc., no specified problem type.</li> <li>data augmentations.</li> </ul> <p>For environments we might consider:</p> <ul> <li>simulated/real</li> <li>data representation: discrete, continuous, categorical, binary, etc.</li> <li>single objective/multi-objective/no-objective</li> <li>partially/fully observable</li> <li>markovian/non-markovian</li> <li>single agent/multi-agent</li> <li>for multi-agent: cooperative/competitive/mixed-mode</li> </ul> <p>Iâ€™ve listed several datasets and environments in the bottom of this post. Ideally, we should train increasingly general ML systems over all of these variations. Still, our training pipelines are very brittle.</p> <p>I propose developing a tool that allows ML praticioners to easily train their agents across many datasets and environments: the Artificial Experience (<code class="language-plaintext highlighter-rouge">ae</code>). <code class="language-plaintext highlighter-rouge">ae</code> should provide minially necesary extensions to extend existing open-source dataset loaders, environments, and hubs. It should be agnostic to the actual training paradigm and tricks (augmentations, experience replay, cirriculum learning, etc.) but itegrate cleanly with tools that do. The following is a declarative description of what I plan to make:</p> <p>The ArtificialExperience environment (<code class="language-plaintext highlighter-rouge">AEEnv</code>) provides a wrapper for multiple environments. Datasets may be wrapped into environments. Turn-based multiagent environments are wrapped into parallel agent cycles environments (you can unwrap this later in your multiagent executor). An AEEnv might look like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="nc">AEEnv</span><span class="p">(</span><span class="n">envs</span><span class="o">=</span><span class="p">[</span>
    <span class="nc">DatasetEnv</span><span class="p">(</span><span class="n">tfds</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">coco</span><span class="sh">'</span><span class="p">)),</span> <span class="c1"># multimodal information
</span>    <span class="nc">DatasetEnv</span><span class="p">(</span><span class="n">hub</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">hub://activeloop/mnist-train</span><span class="sh">'</span><span class="p">))</span>  <span class="c1"># cloud-native data
</span>    <span class="nc">DatasetEnv</span><span class="p">(</span><span class="n">tfds</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">anli</span><span class="sh">'</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">),</span> <span class="c1"># quick customization
</span>    <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">CartPole-v0</span><span class="sh">'</span><span class="p">),</span> <span class="c1"># continous observation, discrete control
</span>    <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">Pong-v0</span><span class="sh">'</span><span class="p">),</span> <span class="c1"># rgb image, discrete actions
</span>    <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">HalfCheetah-v2</span><span class="sh">'</span><span class="p">),</span> <span class="c1"># continuous observation, continuous control
</span>    <span class="n">gym_starcraft</span><span class="p">.</span><span class="n">envs</span><span class="p">.</span><span class="nf">starcraft_base_env</span><span class="p">(),</span> <span class="c1"># starcraft env
</span>    <span class="n">pettingzoo</span><span class="p">.</span><span class="n">atari</span><span class="p">.</span><span class="n">mario_bros_v2</span><span class="p">.</span><span class="nf">env</span><span class="p">()</span> <span class="c1"># multiagent atari env
</span><span class="p">])</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">AEEnv</code> also makes it easy to train on prespecified problem domains with datasets and environments minimally specified by some overlapping hierarchial tag-based system. Not all environments have the <code class="language-plaintext highlighter-rouge">.tag</code> attribute, so those will be ignored. However, the inbuilt list of envionrments should all support this schema. These filters can be changed at any moment between <code class="language-plaintext highlighter-rouge">AEEnv</code> steps. See Appendix A for a list of what I want to support.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="nc">AEEnv</span><span class="p">(</span>
    <span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">domain:text-commonsense</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">domain:image</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">domain:multiagent</span><span class="sh">'</span><span class="p">],</span>
    <span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">domain:reward-free-rl</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">domain:multiagent/atari</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">test:True</span><span class="sh">'</span><span class="p">],</span>
<span class="p">)</span> <span class="c1"># train on text-commonsense (specific), image datasets (broad), and multiagent RL environments (broad) but don't train on the multiagent/atari environment or multiagent environments that don't have a environment specified reward.
</span>
<span class="n">env</span> <span class="o">=</span> <span class="nc">AEEnv</span><span class="p">()</span> <span class="c1"># train on all inbuilt datasets and environments
</span></code></pre></div></div> <p>A <code class="language-plaintext highlighter-rouge">next_env_fn(last_step_data: step, curr_env: env, available_envs: List[env]) -&gt; env</code> determines which environment to sample from at <em>each</em> timestep. This may be a simple â€˜wait until all doneâ€™s are trueâ€™ (for datasets, after all epochs) or it may be a more complex user-designed autocirricula system. An <code class="language-plaintext highlighter-rouge">env_transition_fn(old_env: env, new_env: env) -&gt; NoReturn</code> can be specified to make surface-level model changes when the environment (and hence its interface) changes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># samples a different environment *at every step*. Simple way to train on a diverse lot of datasets within the same problem domain (like images).
</span><span class="n">next_env_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">last_step_data</span><span class="p">,</span> <span class="n">curr_env</span><span class="p">,</span> <span class="n">available_envs</span><span class="p">:</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">available_envs</span><span class="p">)</span> 
<span class="n">env</span><span class="p">.</span><span class="n">next_env_fn</span> <span class="o">=</span> <span class="n">next_env_fn</span>  <span class="c1"># lazy next_env_fn specification
</span>
<span class="c1"># samples a different environment *after every epoch*. Traditional approach to multi-dataset training.
</span><span class="k">def</span> <span class="nf">next_env_fn</span><span class="p">(</span><span class="n">last_step_data</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="n">curr_env</span><span class="p">:</span> <span class="n">env</span><span class="p">,</span> <span class="n">available_envs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">env</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">env</span><span class="p">:</span> 
    <span class="k">if</span> <span class="nf">all</span><span class="p">(</span><span class="n">last_step_data</span><span class="p">[</span><span class="n">agent_name</span><span class="p">].</span><span class="n">done</span> <span class="k">for</span> <span class="n">agent_name</span> <span class="ow">in</span> <span class="n">curr_env</span><span class="p">.</span><span class="n">agent_names</span><span class="p">):</span>
    <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">available_envs</span><span class="p">)</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">curr_env</span>
<span class="n">env</span> <span class="o">=</span> <span class="nc">AEEnv</span><span class="p">(...,</span> <span class="n">next_env_fn</span><span class="o">=</span><span class="n">next_env_fn</span><span class="p">)</span>  <span class="c1"># early next_env_fn specification
</span>
<span class="c1"># builds a new input and output layer for new environments
</span><span class="k">def</span> <span class="nf">env_transition_fn</span><span class="p">(</span><span class="n">old_env</span><span class="p">:</span> <span class="n">env</span><span class="p">,</span> <span class="n">new_env</span><span class="p">:</span> <span class="n">env</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ae</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="nf">all</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="o">==</span><span class="n">y</span><span class="p">,</span> 
        <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">old_env</span><span class="p">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">old_env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">],</span>
        <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">new_env</span><span class="p">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">new_env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">]):</span>
        <span class="c1"># the environments are compatible, no need to change the model
</span>        <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># the environments are incompatible, we need to change the model
</span>        <span class="nf">build_new_input_layer</span><span class="p">(</span><span class="n">new_env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">)</span>
        <span class="nf">build_new_output_layer</span><span class="p">(</span><span class="n">new_env</span><span class="p">.</span><span class="n">action_space</span><span class="p">)</span>
        <span class="k">return</span>
<span class="n">env</span><span class="p">.</span><span class="n">env_transition_fn</span> <span class="o">=</span> <span class="n">env_transition_fn</span>  <span class="c1"># lazy env_transition_fn specification
</span>
<span class="c1"># builds a new input and output layer for new environments
</span><span class="k">def</span> <span class="nf">env_transition_fn</span><span class="p">(</span><span class="n">old_env</span><span class="p">:</span> <span class="n">env</span><span class="p">,</span> <span class="n">new_env</span><span class="p">:</span> <span class="n">env</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">old_env</span><span class="p">,</span> <span class="n">DatasetEnv</span><span class="p">)</span> <span class="ow">and</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">new_env</span><span class="p">,</span> <span class="n">DatasetEnv</span><span class="p">):</span>
        <span class="c1"># the environments are either both datasets or both regular environments, no need to change the model
</span>        <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># the environments are incompatible, we need to change the training pipeline
</span>        <span class="nf">change_training_pipeline</span><span class="p">(</span><span class="n">new_env</span><span class="p">)</span>
        <span class="k">return</span>
<span class="n">env</span> <span class="o">=</span> <span class="nc">AEEnv</span><span class="p">(...,</span> <span class="n">env_transition_fn</span><span class="o">=</span><span class="n">env_transition_fn</span><span class="p">)</span>  <span class="c1"># early env_transition_fn specification
</span></code></pre></div></div> <p>Data is presented at each step as an agent-separated dictionary of namedtuple <code class="language-plaintext highlighter-rouge">Step</code>â€™s as well as meta information about the environment state (or dataset index). A <code class="language-plaintext highlighter-rouge">Step</code> is a nested batch of <code class="language-plaintext highlighter-rouge">observation</code>, <code class="language-plaintext highlighter-rouge">reward</code>, <code class="language-plaintext highlighter-rouge">done</code>, and <code class="language-plaintext highlighter-rouge">information</code> . In most cases, these fields will be <code class="language-plaintext highlighter-rouge">None</code>. For example, datasets do not provide a reward (reward is determined by the training pipeline which is not part of <code class="language-plaintext highlighter-rouge">AEEnv</code>).For supervised learning datasets, the observations include both X and Y while for unsupervised learning datasets, the observations include only X. Also, most datasets and environments will only present information for a single agent. Here are some examples:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evalLoop</span><span class="p">(</span><span class="n">agents</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">is_supervised</span><span class="p">)</span>

    <span class="n">step</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="nf">all</span><span class="p">(</span><span class="n">last_step_data</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">done</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="n">agents</span><span class="p">):</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">agent_name</span><span class="p">:</span> <span class="n">agent</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent_name</span><span class="p">].</span><span class="n">observation</span><span class="p">)</span> 
            <span class="k">for</span> <span class="n">agent_name</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="n">agent_names</span>
        <span class="p">}</span>

        <span class="n">step</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="n">agent_names</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">observation</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">reward</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">done</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">information</span><span class="p">)</span>
</code></pre></div></div> <p>Thereâ€™s a lot more I donâ€™t have time to explain about my plans. Please keep on the lookout for updates: <a href="https://github.com/JacobFV/artificial-experience" target="_blank" rel="noopener noreferrer">https://github.com/JacobFV/artificial-experience</a>.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> Â© Copyright 2025 Jacob F. Valdez. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PFK967EKV4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PFK967EKV4");</script> </body> </html>